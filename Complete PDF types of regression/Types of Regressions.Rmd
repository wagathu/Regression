---
title: "**Different Types of Regression**"
author: "**B.M Njuguna**"
date: "**`r Sys.Date()`**"
output:
  pdf_document:
    pandoc_args: "--listings"
    keep_tex: yes
    include:
      in_header:
      - depth.tex
      - preamble.tex
      - space.tex
    toc_depth: 4
  html_document:
    toc_depth: '4'
    df_print: paged
geometry: margin =2cm
header-includes: \pretitle{\begin{center} \vspace{10cm}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(prompt = TRUE)
knitr::opts_chunk$set(tidy = TRUE)

```

```{=tex}
\newpage 
\tableofcontents
\newpage
```

# 1.0 Regression Analysis

Regression analysis is a set of statistical methods used to identify or estimate the relationship(s) between the **dependent** and **independent** variable(s). It can also be utilized to assess the strength of the relationship and also to model the future relationship between the variables. The dependent variable which is also known as **response variable** is the variable being tested or measured in an experiment, while the independent or the **explanatory or predictor variable** is the variable which is included to the model to explain changes in the dependent variable. In most cases, the dependent variable is denoted by $y$ while the independent variable is usually denoted by $x$.

## 1.1 Types of Regression Analysis

There are several types of regression analysis depending on what you want to achieve, or depending on the nature of the study or the nature of the variables. They include;^[ This paper was compiled by Brian Mwangi Njuguna on 22-08-2022, for acadameic purposes]

1.  Linear Regression

2.  Logistic Regression

3.  Polynomial Regression

4.  Ridge Regression

5.  Quantile Regression

6.  Bayesian Linear Regression

7.  Principal Component Regression

8.  Partial Least Square Regression amongst other types.

\newpage

# 2.0 Linear Regression

A linear regression is a regression model that estimates the relationship between the dependent and the independent variables using a straight line. A linear regression model is as follows; 

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+{...}+\beta_px_{ip}+\epsilon_i$$

Where

-   $y_i$ is the response variable.

-   $\beta_k$ is the $k^{th}$ coefficient, where $\beta_0$ is the constant term in the model.

-   $X_{ij}$ is the $i^{th}$ observation on the $j^{th}$ predictor variable, $j = 1, ..., p.$

-   $\epsilon_i$ is the $i^{th}$ noise term, that is, random error.

If the model includes one predictor variable, that is p=1, then the model is known as a simple linear regression model.

## 2.1 Simple Linear Regression model

A simple linear regression model is of the form;

$$y=\beta_o+\beta_1x$$

where;

$y$ - is the response variable

$\beta_0$ - is the intercept. It refers to the value of $y$ when $x=0$.

$\beta_1$ - is the regression coefficient or the slope. It represents the change in variable $y$ caused by a unit change in the explanatory variable $x$.

It is used to model the relationship between two continuous variables. The assumptions are;

1.  Linearity- The variables $x$ and $y$ must have a linear relationship

2.  The error terms $\epsilon_i$ are independent and that the error is normally distributed with mean 0 and variance $\sigma^2$. That is; $\epsilon_i\sim N(0,\sigma^2)$

For example, We may wish to determine whether advertisement and sales have a linear relationship. Below is a data set containing the budget of advertisement in various platforms including TV,Radios and Newspaper as well as the sales, in 1000\$.

```{r}
## Importing the data set from my library
setwd("D:/Documents/R-Studio Programms/Regression/Linear Regression")
library(readxl)
AdvertisingBudgetandSales<-read_excel("AdvertisingBudgetandSales.xlsx", col_types = c("skip", "numeric","numeric","numeric", "numeric"))
##view first rows of the data set
head  (AdvertisingBudgetandSales)

```

I am going to fit a simple linear regression model where sales is my response variable and advertisement budget in TV is my predictor variable. In r, we use the function *lm()* to fit simple linear regression model as follows;

```{r}
##slr implying simple linear regression
slrTV<-lm(formula =`Sales ($)`~`TV Ad Budget ($)`,data =AdvertisingBudgetandSales)
slrTV
```

From the results above, the model can be written as; $$\hat{y}=7.03259+0.04754\hat{x}$$ or specifically; $$sales=7.03259+0.04754TVAdvert$$ This implies that if there is no budget on TV advertisement, then the sales will stand at \$7032.59, that is (7.03259*1000, since the cost or sales were in 1000 dollars). Then a* $\hat{\beta_1}$ of 0.04754 implies that for a TV advertisement budget equal to 1000 dollars, we expect an increase of 47.54 ( 0.047541000) units in sales. This implies that;

$$sales=7.03259 +0.04754*1000 =54.57259\space units$$

Since we are operating in units of thousand dollars, this represents a total sale of 54572.59 dollars. The fitted regression line is shown below;

```{r}
library(ggplot2)
PLOT1<-ggplot(data = AdvertisingBudgetandSales,aes(`TV Ad Budget ($)`,`Sales ($)`))+geom_point()+stat_smooth(method = lm,se=FALSE)

PLOT1
```

### 2.1.1 Model Assessment<br>

Before using the model to predict future values, we need to check whether;

1.  There is a statistically significant relationship between the predictor variable (TV advert) and the response variable (sales)

2.  The model fits well with the data.

Using the *summary()* function, we will get more insight about the model.

```{r}
summary(slrTV)
```

We use the p-value or the t-statistic to check whether there is a statistically significant relationship between the given predictor variable and the response variable. That is, we check whether or not the $\beta$ coefficient of the predictor variable is significantly different from zero. The hypothesis is formulated as;

$$H_0:\hat{\beta_1}=0\space\space Vs\space\space \space H_1:\hat{\beta_1\neq 0}$$

In this case, the p-value is less than 0.05($\alpha$) hence we reject the null hypothesis and conclude that there is a statistically significant relationship between sales and TV advertisement. We rarely test $\hat{\beta_0}$. The t-statistic is calculated as;

$$t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}$$,

where $SE$ is the standard error of the coefficient $\hat{\beta_1}$.

It is worthy to note that a high t-statistic and a low p-value indicates that the specific predictor variable should be retained in the model,like in our case.

The **standard error** represented by *Std.Error* in the r output above, measures the variability or the accuracy of the $\beta$ coefficients. The standard error is used to calculate the confidence interval of the coefficients. For example, a 95% confidence interval of $\hat{\beta_1}$ is calculated as;

$$\hat{\beta_1}\pm2SE(\hat{\beta_1})$$

The lower limit;

$$\hat{\beta_1}-2SE(\hat{\beta_1})$$

$$0.047537-2* 0.002691=0.042155$$

The upper limit;

$$\hat{\beta_1}+2SE(\hat{\beta_1})$$

$$0.047537+2* 0.002691=0.042155=0.052919$$

Therefore, there is a 95% chance that the interval (0.042155,0.052919) will contain the true value of $\hat{\beta_1}$. Alternatively, it can be done using the *confint()* function in r,

```{r}
confint(slrTV)

```

### 2.1.2 Model Accuracy

The overall quality of the linear regression can be assessed using the following three quantities.

1.  RSE (Residual Standard Error, also known as the model sigma) - it is the standard deviation of the residuals. It represents the average variation of observations points around the regression line. When comparing two model, the model with the lower RSE is the better one. In this case, the RSE is 3.259 which is relatively low.

2.  R-Squared ($R^2$) - It represents the proportion or variation in the data that can be explained by the model, where $0<R^2<1$, but is mostly outlined as a percentage for easier interpretation. The higher the $R^2$, the better the model. In this case, the $R^2= 0.6119$ which is equivalent to 61.19%, implies that 61.19% of the total variation in sales, is explained by the model. As you add more predictor variables, $R^2$ tend to increase, therefore in multiple linear regression, we use the $adjusted\space R^2$, To check the accuracy of the model. In simple linear regression, $R^2$ is the square of the Pearson's correlation coefficient $r$.

```{r}
cor(AdvertisingBudgetandSales$`TV Ad Budget ($)`,AdvertisingBudgetandSales$`Sales ($)`,method = c("pearson"))

0.7822244^2
```

3.  The F-statistic gives the overall significance of the model. Notice that the F-statistic is used to test the overall significance of the model while the t-statistic is used to test the significance of the individual predictor variables. However, in simple linear regression, it has no much use since we only have one predictor variable. It becomes useful while dealing with multiple linear regression. In fact, for any simple linear regression model with 1 degree of freedom, the F-statistic is approximately equal to the square of the t-statistic(of $\hat{\beta_1}$).

```{r}
17.67^2


```

### 2.2. Multiple Linear Regression

Multiple linear regression is an extension of simple linear regression, whereby several predictor variables are used to predict the outcome of the response variable. Assuming that there are three predictor variables, the model can be written as;

$$y_i=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3$$

For easier understanding, let's build a model for estimating sales based on advertisement budged invested on TV, Radio and newspaper. This model can be written as;

$$sales=\beta_0 +\beta_1TV+\beta_2Radio+\beta_3Newspaper$$

In r,it is done as follows;

```{r}

attach(AdvertisingBudgetandSales)
##entering the model;, mlr implying multiple linear regression

mlr<-lm(`Sales ($)`~ `TV Ad Budget ($)`+`Radio Ad Budget ($)`+`Newspaper Ad Budget ($)`,data = AdvertisingBudgetandSales)
mlr
```

Therefore, the model can be written as;

$$sales=2.939+0.046TV+0.189Radio-0.001Newspaper$$

For more analysis of the model, we use the *summary ()* function.

```{r}
summary(mlr)
```

For multiple linear regression, the first step is to check whether the model is significant using the F-statistic or the corresponding p-value. The hypothesis is formulated as;

$$H_0:\hat{\beta_1}=\hat{\beta_2}=\hat{\beta_3}=0 \space\space Vs \space\space H_1:\hat{\beta_i}\neq0 \space for \space i=1,\cdots ,4$$

That is, at least one coefficient is not equal to zero or it is significant. In this case the $p-value =2.2e-16<0.05$, hence we reject the null hypothesis and conclude that the model is significant.

To test the individual significance of the predictor models, we use the t-statistic. If a predictor variable is not statistically significant, then the variable should be dropped.

```{r}
summary(mlr)$coefficients

```

From the output above, TV and Radio predictor variables are statistically significant, but Newspaper is not, since its p-value is greater than 0.05.

The coefficients are interpreted as follows, for a fixed amount of Radio and Newspaper advertisement budget, spending an additional \$1000 on TV advertisement leads to an increase in sales by approximately 0.045764645\*1000= 45.76465 sale units on average. For the Radio advertisement it can be interpreted through the same way. However, for the Newspaper advertisement, it implies that for a fixed amount of TV and Radio advertisement budget, changes in the advertisement budget will not significantly change the sales unit, hence we should remove it from the model, to increase the adjusted R squared.

```{r}
mlr2<-lm(`Sales ($)`~ `TV Ad Budget ($)`+`Radio Ad Budget ($)`,data = AdvertisingBudgetandSales)
summary(mlr2)
```

The model can be written as;

$$sales=2.921+0.046TV+0.188Radio$$

The confidence interval is;

```{r}
confint(mlr2)
```

In multiple linear regression, $R^2$ is the correlation between the observed values of the response variable and the fitted (or predicted) values of the response variable, hence we use the $adjusted\space R^2$ to measure the accuracy of the model. In this case, the $adjusted\space R^2 =0.8962$, which implies that 89.62% of the total variation in the sales, is explained by the model.

### 2.2.1 Sums of Squares

Sums of Squares in regression is a technique used to determine dispersion of data points. They are divided into two.

1.  Sums of Squares due to Regression (SSR)- It is the sum of the differences between the fitted values and the mean of the response variable.

$$\sum_{n=1}^n(\hat{y}-\bar{y})^2$$

2.  Sums of Squares Error (SSE). It is the sum of the differences between the observed values and the predicted or fitted values.

Total sums of squares is the sum of error and regression sums of squares.

$$SST=SSR+SSE $$

Note that, if $SSR=SSE$, then it implies that the regression model captures all the observed variability and is perfect.

3.  Residual Sums of Squares (RSS)- It used to measure the amount of variance in a data set that is not explained by a regression model. It measures the overall difference between the observed data, and the values predicted (or fitted) by the estimation model. The lower the value, the better the model $$\sum_{n=1}^ne_i^2$$

In r, we get the above information using the Analysis of Variance function *anova()* as follows;

```{r}

anova(mlr2)
```

The package *qpcR* in r also have very important functions that are useful in regression analysis.

\newpage

# 3.0 Regression Model Diagonistics

After performing regression analysis, it is important to check whether the model works well for the data in hand. This chapter will explore different ways to check the accuracy of the model. It is important to evaluate how well the model fits the data because it helps you check whether the linear regression assumptions have been met or not. For instance, linear regression assumes that there is a linear relationship between the predictor variable and the response variable which might not be the case. The relationship might be polynomial or logarithmic. In addition, data might contain outliers or extreme values which may affect the regression. This is achieved by checking the distribution of the residual errors. Note that the predicted or the fitted values are the response variable values that you would expect for the given predictor variable values, according to the built regression model. From the scatter plot below, you can see that not all points fall exactly on the regression line. This means that for a given TV or Radio advertisement budget, the observed or the measured values can be different from the predicted or fitted values. The difference is known as **residual errors**, represented by the vertical red lines. The *augment* function from *broom* package gives several metrics useful in regression diagnostic. For easier explanation, I'll use the simple linear regression model.

```{r}
library(tidyverse)
library(broom)
library(ggplot2)
slrTVdiag<-augment(slrTV)
head(slrTVdiag)
```

The plot is as follows;

```{r}
PLOT2<-ggplot(slrTVdiag,aes(`TV Ad Budget ($)`,`Sales ($)`))+geom_point()+geom_segment(aes(xend=`TV Ad Budget ($)`,yend=.fitted),col="red",size=.3)+stat_smooth(method = lm,se=FALSE)
PLOT2
```

As mentioned earlier, the linear regression assumption are linearity, normality of residuals, Homogeneity of residual variance and the independence of the residual error terms.

### 3.1 Diagnostic Plot.

The base function *plot()* or the *autoplot()* function from *ggfortify* package can be used to plot regression diagnostic plots as follows;

```{r}
library(ggfortify)
autoplot(slrTV)

```

**1.The Residual vs Fitted plot**- It used to check linear relationship assumption. An approximate horizontal line without distinct pattern is a good indication of linear relationship.

**2. The Normal Q-Q Plot**- This plot is used to check whether the residuals are normally distributed. The residual terms should follow the straight dashed line to satisfy the assumption.

\*\**3. The Scale-Location Plot*- It used to check whether the residuals have a constant variance( homoscedasticity). A horizontal line with equally spread points is an indication of a constant variance which is not the case in our plot. The plot indicates that the variance of the residuals is heteroscedastic, which should be dealt with.

**4. Residuals vs Leverage Plot**- It is used to check extreme values that may affect the regression.Outliers may affect the interpretation of the model since they increase the RSE of the model.

For our case, the plot shows that there is linear relationship and that the residuals are normally distributed. Let us check the high leverage values and influential values.

```{r}
plot(slrTV,5)
```

A data point has a high leverage if it has an extreme predictor variable values. A data point above the statistic

$$\frac{2(p+1)}{n}$$

(where p is the number of predictors and n is the number of observations) indicates an observation with high leverage. In our case, the statistic is

$$\frac{2*2}{200}=0.02$$

The plot above indicates outliers on the 26, 36 and 179 which have a standardized error below -2, however none exceed a standard deviation of 3. All the observations are below 0.02, hence there are no observations with high leverage.

An influential value is a value which may alter the regression if it is included or excluded in the building of model. Note that not all ouliers are influential values. An observation has influence if its Cook's distance (P. Bruce and Bruce 2017) exceeds;

$$\frac{4}{n-p-1}$$.

```{r}
par(mfrow=c(1,2))
plot(slrTV,4)
plot(slrTV,5)
```

In our case, we do not have influential values, the cooks distance (represented by a red dotted line) is not drawn in the above plot because all the observations are well within the Cooks distance.

From the plot below, there is heteroscedasticity. This can be eliminated by transforming the data in various ways such as log transformation of the response variable.

```{r}
plot(slrTV,3)
```

```{r}
slrTVlog<-lm(log(`Sales ($)`)~`TV Ad Budget ($)`,data = AdvertisingBudgetandSales)
plot(slrTVlog,3)
```

From the plot above, the variance of the residual is homoscedastic.

\newpage

# 4.0 Interaction Effect in Multiple Linear Regression

The multiple linear regression $sales=2.921+0.046TV+0.188Radio$ is also known as an **additive model**. This model only investigates the main effects of the model, where the assumption is that the relationship between one predictor variable and the response variable is independent of the other predictor variables. For example, in the above model, the effect on sales due to TV advertisement is independent of Radio advertisement which might not be true. It can be the case that spending money on TV advertisement may also increase the Radio advertisement effectiveness. In business, this is known as **synergy** while in statistics it is known as *interaction effect*. Generally, the model is written as (assuming we have two independent variables); 

\[\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_1+\hat{\beta_2}x_2+\hat{\beta_3}x_1x_2\]

In r, we can build an interaction model as follows;

```{r}
mlrinteract<-lm(`Sales ($)`~`TV Ad Budget ($)`+`Radio Ad Budget ($)`+`TV Ad Budget ($)`:`Radio Ad Budget ($)`,data = AdvertisingBudgetandSales)


```

Or alternatively
```{r}

mlrinteract<-lm(`Sales ($)`~`TV Ad Budget ($)`*`Radio Ad Budget ($)`,data = AdvertisingBudgetandSales)
## Both ways will build the interaction model
##Let us have glimpse of the model metrics

summary(mlrinteract)
```

From the output above it can be seen that all the coefficients including the interaction coefficient are statistically significant (Note: If the interaction effect is statistically significant, do not try to interpret the predictor variables independently). The model is;

\[sales=6.750220 +0.019101TV+0.028860Radio+  0.001086TV*Radio\]

This interaction is known as **two way interaction** because it is interaction between two independent variables. High order interaction is possible also.

If the Radio advertisement budget is zero, then;

\[sales=6.750+0.019TV\]

The above implies that if Radio budget is zero, then TV advertisement causes an average of 0.019*1000 dollars change in sales

However, if the Radio advertisement budget is one (or $1000 for these case), then;

\[sales=6.750+0.019TV+0.029Tv\]


Therefore;

\[sales=6.750+0.048TV\]

The above implies that a budget of 1000 dollars in Radio advertisement, causes a 0.048*1000 dollars change in sales.

A positive interaction in this case implies that the larger the Radio advertisement budget, the higher the effect of TV advertisement on the sales and similary, the larger the TV advertisement budget, the higher the effect of Radio advertisement on sales.

### 4.1 Additive and Interaction Models Comparison

The **Root Mean Square Error (RMSE)** of the additive model is;
```{r}
library(qpcR)
RMSE(mlr2)

```

While the RMSE of the interaction model is;
```{r}
RMSE(mlrinteract)
```

The lower the RMSE, the better the model. RMSE is the standard deviation of the residuals. It is a metric that tells us the average distance between the predicted or the fitted values and the observed or measured values. It is calculated as;

\[RMSE=\sqrt{\frac{\sum_{i=1}^n P_i-O_i}{n-1}} \]

Also the RMSE is the square root of the Mean Square Error(MSE) where MSE is the average of the squared differences between the observed values and the predicted values. For example;

```{r}
anova(mlrinteract)

##MSE = 0.9 implying that RMSE=sqrt(0.9)

sqrt(0.9)
RMSE(mlrinteract)
```

The Residual Standard Error or the model sigma is the variant of RMSE adjusted for the number of predictors.
Therefore, since the interaction model has lower RMSE, it is the best model. 

Also, the $adjusted\space R^2$ of the interaction model is 0.9673 which is equivalent to 96.73%, while that of the additive model is  0.8962 or 89.62%, implying that the interactive model is better than the additive model, since 96.73% of the total variation in sales is explained by the interactive model.

\newpage

# 5.0 Regression Model Validation.

The commonly used metrics for validation of a regression model are;

1. Root Mean Square Error (RMSE)

2. Residual Standard Error (RSE)

3. R-squared ($R^2$)

4. Mean Absolute Error (MAE)

5. Akaike Information Criterion (AIC) or AICc

6. Bayesian Information Criterion (BIC)

The first three metrics have already been discussed above.

## 5.1 Akaike Information Criterion (AIC)

AIC was developed by a Japanese statistician Hirotugu Akaike, in 1970. AIC penalizes the inclusion of additional variables in the model. It is used to compare various models of the same data and determine which model is the best. The best model is the model with the lowest AIC. Notice that adding more parameters increases the AIC, hence the model with fewer parameters will have the lower AIC. ^[Cavanaugh, J. E., & Neath, A. A. (2019). The Akaike information criterion: Background, derivation, properties, application, interpretation, and refinements. Wiley Interdisciplinary Reviews: Computational Statistics, 11(3), e1460.]

According to AIC, the best model is the model which explains the greatest amount of variation in the dependent variable using the fewest possible independent variables. It is calculated as;

\[AIC=2k-2\ln{L}\]

where;

-$k$ is the number of independent variables.

-$L$ is the log-likelihood estimate (Likelihood that your     model would have produced the observed model)

The default number of independent variables is 2, so if you have one independent variable, then, $k=3$ and so on. 

To use the AIC, you need to build several models and then compare them. For example, we may build several models using the advertisement data set and see which best explains the variations in sale. In this case, I will build separate models for each independent variable, and then compare it with the model for the combined independent variables (TV, Radio and Newspaper). I had already done models for TV advert then TV, Radio and Newspaper advert, as well as TV, Radio advert, hence I'll just proceed to the remaining two separate models for Radio and Newspaper.

```{r}

slrRadio<-lm(`Sales ($)`~`Radio Ad Budget ($)`,data = AdvertisingBudgetandSales)
slrNewspaper<-lm(`Sales ($)` ~`Newspaper Ad Budget ($)`  ,data = AdvertisingBudgetandSales)

```

For clarification, I have named the models as follows;

- slrTV - TV advertisement

- slrRadio - Radio Advertisement

- slrNewspaper - Newspaper advertisement

- mlr - model for combined TV, Radio and Newspaper advert

- mlr2 - combined model for TV and Radio adverts without       newspaper

- mlrinteract - interaction model

I'm going to use the function *aictab()* from the *AICcmodavg* package as follows;

```{r}
##listing the models in a list
Models<-list(slrTV,slrRadio,slrNewspaper,mlr,mlr2,mlrinteract)
##naming the models
Models.names=c("slrTV","slrRadio","slrNewspaper","mlr","mlr2","mlrinteract")
library(AICcmodavg)
##Then use the function
aictab(cand.set=Models, modnames =  Models.names)
```

The best fit model is always listed first. The best model for this study is the Interaction model (Recall that it is the interaction between TV and Radio Adverts). The AICc in the output above contains the model information -the lower the value the better the model. The lowercase "c" implies that it is the AIC of small samples. The Delta_AICc (or Delta_AIC) is the difference between the AICc (or AIC) of the best model and the model being compared. LL is the log likelihood used to calculate the AIC.

## 5.2 Bayesian Information Criterion

It is a method for scoring and selecting a model. It is almost similar to AIC,only that while AIC penalizes the additional parameters, BIC penalizes the complexity.More complex models have higher BICs. The lower the value, the better the model. It is widely used in *logistic Regression* It is calculated as;


\[BIC=-2L+\ln{N}*K\]

Where;
-$k$ is the number of independent variables.

-$L$ is the log-likelihood estimate (Likelihood that your     model would have produced the observed model)
- $N$ is the number of observations.

**Note that AIC and BIC are best used in models fit by Maximum Likelihood Estimation framework**

## 5.3  Mean Absolute Error (MAE)

MAE is a loss function used for regression. The loss is the mean over the absolute differences of the observed values and the predicted values. The lower the value the better the model. It is calculated as;

\[MAE=\frac{1}{N}\sum_{i=1}^N|y-\hat{y_i}|\]

\newpage

# 6.0 Logistic Regression

**Logistic Regression** is used to predict the category or class of individuals using one or multiple predictor variables. Logistic regression estimates the probability of an event occurring, therefore, since the outcome is probability, the dependent variable is bounded between 0 and 1. It is used to predict the outcome of a binary (such as yes or no) based on past values of the data. Logistic regression belongs to the family of **Generalized Linear Models (GLM)** which was built to extend the Linear Regression Model. Logistic Regression is also known as *binary logistic regression* ,*binomial logistic regression* or *logit regression*. The model was initially introduced by Joseph Berkson in 1944. The *response variable* which I will refer as $Y$ in this paper, is parametarized by $0\space or\space 1$. Traditionally, $1$ is indicates a *success* while $0$ indicates a *failure* or *lack of success*. $1$ can also be thought of having a certain characteristic, condition, requirement or property while $0$ can be thought of having certain characteristics or properties. This type of regression is widely used in epidemiological data analysis. For example, we might have a research question such as; *what is the relationship between one or more exposure variable(s) say E to a disease or illness outcome D*. Let us further take an example of smoking habits and Coronary Heart Disease(CHD), where the question is to which extent is CHD related with smoking. Here, $1$ will represent smoker and $0$ non smoker for the case of the independent variable smoking. Also, $1$ will represent *diseased or having CHD* and $0$ will represent *not diseased or not having CHD* , for the case of the response variable.

Other independent variables such as age, race and sex are known as **control variables** ($C_s$) . These control variables together with the binary variables $E_S$ form a collection of independent variables which are used to predict the outcome of the response variable. More generally, as usual, the independent variables are represented with $X$ regardless of whether they are $C_s$ or $E_s$ -*Note that the subscript s is used to represent prural or many variables*. Also, in logistic regression the dependent variable is always a binary outcome.

It is important to note that logistic regression is based on the logistic function below;

$$f(z)=\frac{1}{1+e^{-z}}\space\space for\space\space -\infty<z<\infty $$

The graph of this function is shown below;

```{r}
eq=function(z){
  1/(1+exp(-z))
}
library(ggplot2)
base<-ggplot()+xlim(-5,5)+ylim(-1,1)
base+geom_function(fun=eq,col="red")+geom_hline(yintercept =c( 0,1),linetype="dotted")+geom_vline(xintercept = 0,linetype="dotted")

```

Notice that for $z=-\infty$, then $f(z)=0$ and for $z=\infty$,then $f(z)=1$. Thus the value of $f(z)$ ranges from zero to one regardless of the value of $z$, which is the primary reason as to why logistic regression is popular. As mentioned earlier, the response variable to be predicted is a probability, and as you know probability ranges from zero to one, hence this model is suitable.

The elongated **S** shape of the function appeals the epidemiologists if $z$ represents an index that combines the contribution of several risk factors and $f(z)$ represents the risk for a given value of $z$. As in most diseases and risk factors, the logistic function shows that at low level exposure, the individual's risk is minimal until it reaches a certain threshold after which the risk increases rapidly, to a certain threshold also where the risk remains extremely high.

\newpage

## 6.1 The Logistic Model

To obtain the logistic model, we write $z$ as a linear sum of $x\space\space and \space\space \beta_i$ as follows;

$$z=\beta_0+\beta_1x_1+\beta_2x_2+\dots\beta_nx_n$$

Then, substituting for $z$ in the logistic function above, we get;

$$f(z)=\frac{1}{(1+e^{-(\beta_0+\sum_{i=1}^n\beta_i x_i)}}$$

Assuming that $D$ is the disease whose probability is being modeled, then we denote $f(z)$ as follows;

$$\mathbb{P}(D|x_1,x_2\dots x_i)=\frac{1}{(1+e^{-(\beta_0+\sum_{i=1}^n\beta_i x_i)}}$$

By multiplication, it can be shown that;

$$\frac{\mathbb{P}(D)}{1-\mathbb{P}(D)}=e^{(\beta_0+\beta_1x1+\dots+\beta_nx_n)}$$

Taking log on both sides we get; $$\log(\frac{\mathbb{P}(D)}{1-\mathbb{P}(D)})=\beta_0+\beta_1x_1+\dots+\beta_nx_n$$

The quantity $\log\{\frac{\mathbb{P}(D)}{1-\mathbb{P}(D)}\}$ is known as **log-odd** or the **logit**. The **odd** refer to the likelihood of an event occurring. It can also be seen as the ratio of "success" to "non-success".

### 6.1.1 Simple Logistic Model

The simple logistic model is used to estimate the probability of class membership based on one predictor or independent variable. The model is written as;

$$\mathbb{P}(D|x_1)=\frac{1}{(1+e^{-(\beta_o+\beta_1x_1)})}$$

To understand this type of regression, let us use an example where let the disease of interest $D$ be $CHD$ or Coronary Heart Disease. We wish to find out whether a certain gender increases the risk of contracting CHD. I am going to use a data set publicly available on *Kaggle*. The data set is from a study of cardiovascular study on residents of the town of Framingham, Massachusetts, which recorded whether the persons in the study contracted CHD.

```{r}
##Importing the data
setwd("D:/Documents/R-Studio Programms/Regression/Logistic Regression")
library(readxl)
CHD<-read_excel("CHD.xlsx")
 
head(CHD)

##Data Preparation
CHD$Gender<-factor(CHD$Gender,levels = c(0,1),labels = c("Female","Male"))
CHD$currentSmoker<-factor(CHD$currentSmoker,levels = c(0,1),labels = c("No","Yes"))
CHD$Hyp<-factor(CHD$Hyp,levels = c(0,1),labels = c("No","Yes"))

CHD$CHD<-factor(CHD$CHD,levels = c(0,1),labels = c("No","Yes"))
model1<-glm(CHD~Gender,family = binomial,data = CHD)
summary(model1)
```

In logistic model, a negative intercept ($\beta_0$) implies that the probability of having the disease or the outcome is below 0.5. A positive intercept implies that the probability of having the disease or the outcome is greater than 0.5, while an intercept equal to zero implies that the probability is approximately equal to 0.5.

From the output above, our model can be written as;

$$\mathbb{P}(CHD|gender)=\frac{1}{(1+e^{-(1.957+0.511gender)})}$$

The probability of CHD infection given that you are a male is;

$$\mathbb{P}(CHD|gender)=\frac{1}{(1+e^{-(-1.957+0.511*1)})}=0.191$$

And the probability of CHD infection given that you are a female is;

$$\mathbb{P}(CHD|gender)=\frac{1}{(1+e^{-(1.951+0.492*0)})}=0.124$$

From the above calculations we can conclude that the risk of having CHD is higher for males than for females. Then if we divide the predicted risk of males by the predicted risk of females as shown below, we obtain the **risk ratio estimate** $\hat{RR}$.

$$\frac{\mathbb{P}(CHD|male)}{\mathbb{P}(CHD|female)}=\frac{0.191}{0.124}=1.540$$

Thus using the the fitted model above we find that risk of males contracting CHD is one and half times the risk of females contracting CHD. More information can be obtained by the summary function in r.

```{r}

summary(model1)

```

The **null deviance** above is a concept in generalized linear models that measures the fitted generalized model against a perfect model known as the **saturated model**. It is a generalization of the total sums of squares in a linear model. The null deviance shows how well the model predicts the response variable with the intercept only. The smaller the Null Deviance, the better the model.

The **residual deviance** shows how well a model can predict the response variable with more than -say $p$- predictors. To determine the usefulness of the model we can calculate a $\chi^2$ statistic with $p$ (number of predictor variables) degrees of freedom as follows; Note that, the lower the value of residual deviance, the better the model (Obviously)

$$\chi^2_{(p)}=Null\space deviance\space -\space Residual\space deviance$$

After which we find the p-value associated with the statistic, if it is less than $\alpha=0.05$ (for our case), then the model is useful or significant. We only have one predictor variable, thus the degrees of freedom = 1.

$$\chi^2_{(1)}= 3257.3-3225.3=32$$

```{r}
pchisq(q=32,df=1,lower.tail = F)
```

Thus since the p-value is less than $\alpha$, our model is useful.

### 6.1.2 Multiple Logistic Regression

The procedure for multiple logistic regression is the same as for the simple logistic regression. A multiple logistic regression is logistic regression where more than one predictor variables are used to predict the response variable. For example, we may wish to know the effect of age, hypertension, gender, heart rate, smoking habit, BMI level, cholesterol levels and glucose levels on contracting CHD. In r, we build a multiple logistic model as follows;

```{r}
model2<-glm(CHD~Gender+age+heartRate+cigsPerDay+Hyp+glucose+Chol+BMI,family = binomial,data = CHD)
summary(model2)
```

From the output above, we can see that most predictor variables are significant at $1\%$ level of significance. However, the predictor variables named heartRate and BMI levels are not significant and should be eliminated to increase the accuracy of the model. This elimination can be automatically using statistical techniques, including **step-wise regression** and **penalized regression**.

```{r}
model3<-glm(CHD~Gender+age+cigsPerDay+Hyp+glucose+Chol,family = binomial,data = CHD)
summary(model3)
 

```

Thus our model can be written as;

$$\mathbb{P}(CHD|gender,\dots)=\frac{1}{(1+e^{-(-7.518+0.504Gender+0.071Age+0.020CigsPerDay+0.660Hyp+0.008glucose+0.003Cholestrol)})}$$

From the output above, we see that all the $\beta$ coefficients of predictor variables are positive which implies that if any of the predictor variables is increased, the probability of having CHD increases (This applies for the control predictor variables) We can also calculate the odds ratio by taking the exponential of the predictor variables. For example, the $\beta$ coefficient for glucose is $0.008$ which implies that a one unit increase in glucose will increase the odds of having CHD by $e^{0.008}=1.008$ times. Number of iterations is just a measure of how long it took to fit your model and you can safely ignore it.

Note that, we only estimate the Risk Ratio using the logistic regression if a follow up study was conducted. However, in the case of Cross-sectional and case control study, we cannot use the logistic regression to estimate the individual risks but only the odds ratio. In follow-up study, it is commonly preferred to estimate the Risk Ratio rather than the Odds Ratio.

\newpage

# 7.0 Multinomial Logistic Regression

A **Multinomial Logistic Regression** is an extension of the logistic regression which is used when the outcome involves more than two classes. It allows for more than two categories of the dependent or the independent variable. Multinomial logistic regression is often considered an attractive analysis because it does not assume normality, linearity or homoscedasticity. However, it does have an assumption of independence of choices in the dependent variable which states that the choice of or membership in one category is not related to the choice or membership of another category(IIA). This assumption can be tested using the **Hausman-McFadden test**, in r, we use the function **hmftest**, from **mlogit** package. Given that the categorical response variable $Y$ has more than two possible levels, namely ${1,\dots,J}$ and further given that we have $X_1,X_2,\dots,X_P$, then the multiple logistic regression models the probability of each level $j$ of $Y$, by;

$$p_J(\mathbf{x}):=\mathbb{P}(Y=j|X_1=x_p,\dots,X_p=x_p)=\frac{1}{(1+\sum_{\ell=1}^{J-1}e^{(\beta_{0\ell}+\beta_{1\ell}X_1+\dots+\beta_{p\ell}X_p)}}$$

The multinomial regression has an interesting interpretation of logistic regression. For example taking the quotients;

$$\frac{p_j(\mathbf{x})}{p_J(\mathbf{x})}= e^{\beta_{0j}+\beta_{1j}X_1+\dots+\beta_{pj}X_p}\space for\space j=1,\dots,J-1$$

Then taking logarithms on both sides we get;

$$\log{(\frac{p_j(\mathbf{x})}{p_J(\mathbf{x})})}=\beta_{0j}+\beta_{1j}X_1+\dots+\beta_{pj}X_p$$

Thus, if the probabilities on LHS added to one, we would have gotten a log-odds and hence the logistic regression for $Y$, which is not the case. Therefore, due to this effect, we have the *ratios* or the *log-ratios* of the non-complimentary probabilities. Further, $e^{\beta_{0j}}$ is the ratio between $p_j(\mathbf{0})$ and $p_J(\mathbf{0})$, representing the probabilities of $Y=j$ when $X_1=X_2=\dots=X_p=0$.[^1] Note the following;

[^1]: Note that the multinomial regression is like a set of $J-1$ independent logistic regressions for the probability of $Y=j$ versus the probability of reference $Y=J$

1.  If $e^{\beta_{0j}}>1$ and $\beta_{0j}>0$, then the $Y=j$ is more likely than $Y=J$

2.  However, if $e^{\beta_{0j}}<1$ and $\beta_{0j}<0$, then $Y=j$ is less likely than $Y=J$

3.  $e^{\beta_{\ell j}}$ is the **multiplicative** increment of the ratio between $p_j(\mathbf{x})$ and $p_J(\mathbf{x})$ for an increment in one unit of $X_\ell=x_\ell$, provided that the remaining variables; $X_1,\dots,X_{\ell-1},X_{\ell+1},\dots,X_p$ do not change.

4.  If $e^{\beta_{\ell j}}>1$ and equivalently, $\beta_{\ell j}>0$, then $Y=j$ becomes more likely than $Y=J$ for each increment in $X_j$.

5.  if $e^{\beta_{\ell j}}<1$ and $\beta_{\ell j}<00$, then $Y=j$ becomes less likely than $Y=J$ for each increment in $X_j$.

## 7.1 Examples of Multinomial Logistic Regression

1.  People's occupation choices might be influenced by their parents' occupation and their own educational level. We can model or study the relationship between one's occupation, educational level and the father's or mother's occupation. The outcome or the response variable will be the occupation choices which consists of occupational categories.

2.  A biologist may be interested in the food choices an alligator makes. Adult alligators might have different choices from younger ones. Here, the response variable will be the food choices while the predictor variables can be the size of alligators (which is continuous) and other environmental factors.

3.  High school students makes a program choice among general program, vocational program and academic program. Their choice might be modeled based on their writing score and social economic status.

In r, there are various packages with inbuilt functions for building a multinomial logistic regression, but am going to use the **nnet package**. Let's us model the student program choice based on their writing score and social economic status. I'm first going to download the data set known as **hsbdemo** from [stats idre website](https://stats.idre.ucla.edu)

```{r}
require(foreign)
require(nnet)
require(ggplot2)
require(reshape2)

##downloading the data
Student <- read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")

##A glimpse of the first few rows of the data
head(Student)


```

The response variable will be the program -named prog in the data set- while the predictor variables will be social economic status -named ses- which is a three-level categorical variable, writing score -named write-a continuous variable.

First, we need to choose our level of outcome that we wish to use as our baseline and then specify this with **relevel** function.

```{r}
Student$prog2<-relevel(Student$prog,ref = "academic")
```

After specifying our baseline level of outcome, We can now use the function **multinom** from the **nnet** package to build the model.

```{r}
mlt<-multinom(prog2~ses+write,data = Student)

summary(mlt)

```

The *multinom* function, does not automatically calculate the p-values, hence we will do it manually as follows;

```{r}
z <- summary(mlt)$coefficients/summary(mlt)$standard.errors
z
```

We can then proceed to calculate the 2-tailed z-test

```{r}
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p
```

### 7.1.1 Interpretation of the output

The model summary output has a block of coefficients and a block of standard errors. Focusing on the block of coefficients, each row contains a model equation. The first row compares the general program to our baseline academic program while the second row compares the vocation programs to academic program. Let the coefficients from the first row be $\hat{\beta_1}$ and further let coefficients from the second row be $\hat{\beta_2}$, then we can write our model as;

$$\ln(\frac{\mathbb{P}(Program=general)}{\mathbb{P}(Program=academic)})=\hat{\beta_{10}}+\hat{\beta_{11}}(ses=middle)+\hat{\beta_{12}}(ses=high)+\hat{\beta_{13}}write$$

$$\ln(\frac{\mathbb{P}(Program=general)}{\mathbb{P}(Program=academic)})=2.852-0.533(ses=middle)-1.163(ses=high)-0.058write$$

This can be interpreted as:

-   A one unit increase in write leads to 0.058 decrease in the log odds of being in general program versus academic program.

-   The log odds of being in general program vs academic program decreases by an average of 1.163 if the student has a high social economic status.Or, a high class social economic status student is less likely to be in the general program than to be in the academic program

-   The log odds of being in general program vs academic program decreases by 0.533 if the student has a middle social economic status. Or, a middle class social economic status student is less likely to be in the general program than to be in the academic program (although not significant).

-   Since our $\hat{\beta_{10}}>0$, then, a student is more likely to be in the general program than to be in the academic program, in the absence of other predictor variables.

Now let the coefficients from the second row be $\hat{\beta_2}$, then we can write the model as;

$$\ln(\frac{\mathbb{P}(Program=vocation)}{\mathbb{P}(Program=academic)})=\hat{\beta_{20}}+\hat{\beta_{21}}(ses=middle)+\hat{\beta_{22}}(ses=high)+\hat{\beta_{23}}write$$

$$\ln(\frac{\mathbb{P}(Program=vocation)}{\mathbb{P}(Program=academic)})=5.219+0/291(ses=middle)-0.983(ses=high)-0.114write$$

This can be interpreted as;

-   A one unit increase in write leads to 0.114 decrease in the log odds of being in vocation program versus academic program.

-   The log odds of being in vocation program vs academic program decreases by 0.983 if the student has a high class social economic status. Or, a high class social economic status student is less likely to be in the vocation program than to be in the academic program.

-   The log odds of being in vocation program vs academic program increases by 0.291 if the student has a middle social economic status. Or, a middle class social economic status student is more likely to be in the vocation program than to be in the academic program (although not statistically significant looking at its p-value).

-   Since our $\hat{\beta_{20}}>0$, then, a student is more likely to be in the vocation program than to be in the academic program, in the absence of other predictor variables.

Then the ratio of the probability of choosing one variable outcome over another is called the relative risk. We obtain it by exponentiation of the Right Hand Side of the model.

```{r}

exp(coef(mlt))
```

### 7.1.2 Assumptions of the Multinomial Logistic Regression

1.  The **Independence of the Irrelevant Alternatives (IIA)**- The IIA assumption means that deleting or removing alternative outcome (response) categories does not affect the odds among the remaining outcomes.

2.  Sample size- The multinomial logistic regression uses the Maximum Likelihood Estimation method which requires a large sample size. It also uses multiple equations which implies that it requires a larger sample size than what a binary logistic regression would require.

3.  Complete or quasi-complete separation- Complete separation means that the outcome variable separate a predictor variable completely, leading perfect prediction by the predictor variable.

You can also use the predicted or estimated probabilities to help you understand your model better as follows;

```{r}
head(pp <- fitted(mlt))
```

\newpage

# 8.0 Ordinal Logistic Regression

Ordinal logistic regression is a statistical analysis method used to model the relationship between an **ordinal response variable** and one or more predictor variable(s). Ordinal data is a type of qualitative type of data with a natural ordered scale e.g the level of income can be low, middle or high. The flowchart below shows the types of data.

```{r}

```

In r, we can use the **polr** command from **Mass** package to build an Ordinal logistic regression. The command name comes from proportional odds logistic regression.

\newpage

# 9.0 Polynomial Regression

In some cases, the relationship between the response variable $y$ and the independent or predictor variable or variables might not be linear. In a such a case, we cannot apply the linear regression analysis as the assumption of linearity is violated. Thus, the **Polynomial Regression** is a type of regression whereby the relationship between the response variable and the predictor variables is modeled as the $n^{th}$ degree polynomial. The polynomial regression fits a non-linear relationship between the values of the independent variable $X$ and the conditional mean of $y$ which is denoted as $\mathbf{E}(y|x)$. Although the polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear in the sense that the conditional mean of $y$ i.e $\mathbf{E}(y|x)$ is linear to the unknown to the parameters estimated from the data, hence it is referred to as a special case of multilple linear regression.

With polynomial regression, data is approximated using a polynomial equation of degree $n$ written as;

$$\mathbf{f(x)}=\alpha_0+\alpha_1x+\alpha_2x_1+\dots+\alpha_nx^n$$

where $\alpha$ is the set of coefficients.

Now the polynomial regression equation can be written as;

$$y=\beta_0+\beta_1x_i+\beta_2x_i^2+\dots+\beta_nx_i^m+\epsilon_i \\for\space i=1,2,3,\dots,n$$

The above model can be expressed matrix form in terms of a design matrix $\mathbb{x}$, response vector $\vec{y}$, the parameter vector $\vec{\beta}$ and the random vector $\vec{\epsilon }$, as follows;

$$\begin{bmatrix}y_1\\y_2\\y_3\\\vdots\\y_n \end{bmatrix}=\begin{bmatrix}1&x_1&x_2^2&\dots&x_n^m\\1&x_2&x_2^2&\dots&x_2^m\\1&x_3&x_3^2&\dots&x_3^m\\\vdots&\vdots&\vdots&\ddots&\vdots\\1&x_n&x_n^2&\dots&x_n^m\end{bmatrix}\begin{bmatrix}\beta_0\\\beta_1\\\beta_3\\\vdots\\\beta_m\end{bmatrix}\begin{bmatrix}\epsilon_1\\\epsilon_2\\\epsilon_3\\\vdots\\\epsilon_n\end{bmatrix}$$

Which can be written as;

$$\vec{y}=\mathbf{x}\vec{\beta}+\vec{\epsilon}$$

The polynomial coefficients $\beta$ can be estimated using the **ordinary Least Square** as follows;

$$\vec{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}\vec{y}$$

It is often difficult to interpret individual polynomial regression coefficients since the underlying monomials are highly correlated for example if $x$ is **Uniformly distributed**,then $x$ and $x^2$ have a high correlation of 0.97. Therefore, it is generally informative to consider the fitted regression function as a whole.[^1]

[^1]: monimal *in plural monomials* is an algebraic expression consisting of one term.

In r, we use the function **poly()** which is in the basic syntax to fit a polynomial regression model to data.
































































