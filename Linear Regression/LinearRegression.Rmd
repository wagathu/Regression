---
title: "**Linear Regression Analysis**"
author: "**B.M Njuguna**"
date: "**`r Sys.Date()`**"
output:
  pdf_document:
   
    pandoc_args: --listings
    keep_tex: true       
    include:
      in_header: ["depth.tex","preamble.tex","space.tex"]
    
    toc_depth: 4
   
geometry: margin =2cm
header-includes:
- \pretitle{\begin{center} \vspace{10cm}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(prompt = TRUE)
knitr::opts_chunk$set(tidy = TRUE)
```

```{=tex}
\newpage
\tableofcontents
\newpage
```
## 1. Regression Analysis

Regression analysis is a set of statistical methods used to identify or estimate the relationship(s) between the **dependent** and **independent** variable(s). It can also be utilized to assess the strength of the relationship and also to model the future relationship between the variables. The dependent variable which is also known as **response variable** is the variable being tested or measured in an experiment, while the independent or the **explanatory or predictor variable** is the variable which is included to the model to explain changes in the dependent variable. In most cases, the dependent variable is denoted by $y$ while the independent variable is usually denoted by $x$.

## 1.1 Types of Regression Analysis

There are several types of regression analysis depending on what you want to achieve, or depending on the nature of the study or the nature of the variables. They include;[^1]

[^1]:  This paper was compiled by Brian Mwangi Njuguna on 22-08-2022, for acadameic purposes

1.  Linear Regression

2.  Logistic Regression

3.  Polynomial Regression

4.  Ridge Regression

5.  Quantile Regression

6.  Bayesian Linear Regression

7.  Principal Component Regression

8.  Partial Least Square Regression amongst other types.

## 2. Linear Regression

A linear regression is a regression model that estimates the relationship between the dependent and the independent variables using a straight line. A linear regression model is as follows;

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+{...}+\beta_px_{ip}+\epsilon_i$$

Where

-   $y_i$ is the response variable.

-   $\beta_k$ is the $k^{th}$ coefficient, where $\beta_0$ is the constant term in the model.

-   $X_{ij}$ is the $i^{th}$ observation on the $j^{th}$ predictor variable, $j = 1, ..., p.$

-   $\epsilon_i$ is the $i^{th}$ noise term, that is, random error.

If the model includes one predictor variable, that is p=1, then the model is known as a simple linear regression model.

### 2.1 Simple Linear Regression model

A simple linear regression model is of the form;

$$y=\beta_o+\beta_1x$$

where;

$y$ - is the response variable

$\beta_0$ - is the intercept. It refers to the value of $y$ when $x=0$.

$\beta_1$ - is the regression coefficient or the slope. It represents the change in variable $y$ caused by a unit change in the explanatory variable $x$.

It is used to model the relationship between two continuous variables. The assumptions are;

1.  Linearity- The variables $x$ and $y$ must have a linear relationship

2.  The error terms $\epsilon_i$ are independent and that the error is normally distributed with mean 0 and variance $\sigma^2$. That is; $\epsilon_i\sim N(0,\sigma^2)$

For example, We may wish to determine whether advertisement and sales have a linear relationship. Below is a data set containing the budget of advertisement in various platforms including TV,Radios and Newspaper as well as the sales, in 1000\$.

```{r}
## Importing the data set from my library
library(readxl)
AdvertisingBudgetandSales<-read_excel("AdvertisingBudgetandSales.xlsx", col_types = c("skip", "numeric","numeric","numeric", "numeric"))
##view first rows of the data set
head  (AdvertisingBudgetandSales)

```

I am going to fit a simple linear regression model where sales is my response variable and advertisement budget in TV is my predictor variable. In r, we use the function *lm()* to fit simple linear regression model as follows;

```{r}
##slr implying simple linear regression
slrTV<-lm(formula =`Sales ($)`~`TV Ad Budget ($)`,data =AdvertisingBudgetandSales)
slrTV
```

From the results above, the model can be written as; $$\hat{y}=7.03259+0.04754\hat{x}$$ or specifically; $$sales=7.03259+0.04754TVAdvert$$ This implies that if there is no budget on TV advertisement, then the sales will stand at \$7032.59, that is (7.03259*1000, since the cost or sales were in 1000 dollars). Then a* $\hat{\beta_1}$ of 0.04754 implies that for a TV advertisement budget equal to 1000 dollars, we expect an increase of 47.54 ( 0.047541000) units in sales. This implies that;

$$sales=7.03259 +0.04754*1000 =54.57259\space units$$

Since we are operating in units of thousand dollars, this represents a total sale of 54572.59 dollars. The fitted regression line is shown below;

```{r}
library(ggplot2)
PLOT1<-ggplot(data = AdvertisingBudgetandSales,aes(`TV Ad Budget ($)`,`Sales ($)`))+geom_point()+stat_smooth(method = lm,se=FALSE)

PLOT1
```

#### 2.1.1 Model Assessment<br>

Before using the model to predict future values, we need to check whether;

1.  There is a statistically significant relationship between the predictor variable (TV advert) and the response variable (sales)

2.  The model fits well with the data.

Using the *summary()* function, we will get more insight about the model.

```{r}
summary(slrTV)
```

We use the p-value or the t-statistic to check whether there is a statistically significant relationship between the given predictor variable and the response variable. That is, we check whether or not the $\beta$ coefficient of the predictor variable is significantly different from zero. The hypothesis is formulated as;

$$H_0:\hat{\beta_1}=0\space\space Vs\space\space \space H_1:\hat{\beta_1\neq 0}$$

In this case, the p-value is less than 0.05($\alpha$) hence we reject the null hypothesis and conclude that there is a statistically significant relationship between sales and TV advertisement. We rarely test $\hat{\beta_0}$. The t-statistic is calculated as;

$$t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}$$,

where $SE$ is the standard error of the coefficient $\hat{\beta_1}$.

It is worthy to note that a high t-statistic and a low p-value indicates that the specific predictor variable should be retained in the model,like in our case.

The **standard error** represented by *Std.Error* in the r output above, measures the variability or the accuracy of the $\beta$ coefficients. The standard error is used to calculate the confidence interval of the coefficients. For example, a 95% confidence interval of $\hat{\beta_1}$ is calculated as;

$$\hat{\beta_1}\pm2SE(\hat{\beta_1})$$

The lower limit;

$$\hat{\beta_1}-2SE(\hat{\beta_1})$$

$$0.047537-2* 0.002691=0.042155$$

The upper limit;

$$\hat{\beta_1}+2SE(\hat{\beta_1})$$

$$0.047537+2* 0.002691=0.042155=0.052919$$

Therefore, there is a 95% chance that the interval (0.042155,0.052919) will contain the true value of $\hat{\beta_1}$. Alternatively, it can be done using the *confint()* function in r,

```{r}
confint(slrTV)

```

#### 2.1.2 Model Accuracy

The overall quality of the linear regression can be assessed using the following three quantities.

1.  RSE (Residual Standard Error, also known as the model sigma) - it is the standard deviation of the residuals. It represents the average variation of observations points around the regression line. When comparing two model, the model with the lower RSE is the better one. In this case, the RSE is 3.259 which is relatively low.

2.  R-Squared ($R^2$) - It represents the proportion or variation in the data that can be explained by the model, where $0<R^2<1$, but is mostly outlined as a percentage for easier interpretation. The higher the $R^2$, the better the model. In this case, the $R^2= 0.6119$ which is equivalent to 61.19%, implies that 61.19% of the total variation in sales, is explained by the model. As you add more predictor variables, $R^2$ tend to increase, therefore in multiple linear regression, we use the $adjusted\space R^2$, To check the accuracy of the model. In simple linear regression, $R^2$ is the square of the Pearson's correlation coefficient $r$.

```{r}
cor(AdvertisingBudgetandSales$`TV Ad Budget ($)`,AdvertisingBudgetandSales$`Sales ($)`,method = c("pearson"))

0.7822244^2
```

3.  The F-statistic gives the overall significance of the model. Notice that the F-statistic is used to test the overall significance of the model while the t-statistic is used to test the significance of the individual predictor variables. However, in simple linear regression, it has no much use since we only have one predictor variable. It becomes useful while dealing with multiple linear regression. In fact, for any simple linear regression model with 1 degree of freedom, the F-statistic is approximately equal to the square of the t-statistic(of $\hat{\beta_1}$).

```{r}
17.67^2


```

### 2.2. Multiple Linear Regression

Multiple linear regression is an extension of simple linear regression, whereby several predictor variables are used to predict the outcome of the response variable. Assuming that there are three predictor variables, the model can be written as;

$$y_i=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3$$

For easier understanding, let's build a model for estimating sales based on advertisement budged invested on TV, Radio and newspaper. This model can be written as;

$$sales=\beta_0 +\beta_1TV+\beta_2Radio+\beta_3Newspaper$$

In r,it is done as follows;

```{r}

attach(AdvertisingBudgetandSales)
##entering the model;, mlr implying multiple linear regression

mlr<-lm(`Sales ($)`~ `TV Ad Budget ($)`+`Radio Ad Budget ($)`+`Newspaper Ad Budget ($)`,data = AdvertisingBudgetandSales)
mlr
```

Therefore, the model can be written as;

$$sales=2.939+0.046TV+0.189Radio-0.001Newspaper$$

For more analysis of the model, we use the *summary ()* function.

```{r}
summary(mlr)
```

For multiple linear regression, the first step is to check whether the model is significant using the F-statistic or the corresponding p-value. The hypothesis is formulated as;

$$H_0:\hat{\beta_1}=\hat{\beta_2}=\hat{\beta_3}=0 \space\space Vs \space\space H_1:\hat{\beta_i}\neq0 \space for \space i=1,\cdots ,4$$

That is, at least one coefficient is not equal to zero or it is significant. In this case the $p-value =2.2e-16<0.05$, hence we reject the null hypothesis and conclude that the model is significant.

To test the individual significance of the predictor models, we use the t-statistic. If a predictor variable is not statistically significant, then the variable should be dropped.

```{r}
summary(mlr)$coefficients

```

From the output above, TV and Radio predictor variables are statistically significant, but Newspaper is not, since its p-value is greater than 0.05.

The coefficients are interpreted as follows, for a fixed amount of Radio and Newspaper advertisement budget, spending an additional \$1000 on TV advertisement leads to an increase in sales by approximately 0.045764645\*1000= 45.76465 sale units on average. For the Radio advertisement it can be interpreted through the same way. However, for the Newspaper advertisement, it implies that for a fixed amount of TV and Radio advertisement budget, changes in the advertisement budget will not significantly change the sales unit, hence we should remove it from the model, to increase the adjusted R squared.

```{r}
mlr2<-lm(`Sales ($)`~ `TV Ad Budget ($)`+`Radio Ad Budget ($)`,data = AdvertisingBudgetandSales)
summary(mlr2)
```

The model can be written as;

$$sales=2.921+0.046TV+0.188Radio$$

The confidence interval is;

```{r}
confint(mlr2)
```

In multiple linear regression, $R^2$ is the correlation between the observed values of the response variable and the fitted (or predicted) values of the response variable, hence we use the $adjusted\space R^2$ to measure the accuracy of the model. In this case, the $adjusted\space R^2 =0.8962$, which implies that 89.62% of the total variation in the sales, is explained by the model.

#### 2.2.1 Sums of Squares

Sums of Squares in regression is a technique used to determine dispersion of data points. They are divided into two.

1.  Sums of Squares due to Regression (SSR)- It is the sum of the differences between the fitted values and the mean of the response variable.

$$\sum_{n=1}^n(\hat{y}-\bar{y})^2$$

2.  Sums of Squares Error (SSE). It is the sum of the differences between the observed values and the predicted or fitted values.

Total sums of squares is the sum of error and regression sums of squares.

$$SST=SSR+SSE $$

Note that, if $SSR=SSE$, then it implies that the regression model captures all the observed variability and is perfect.

3.  Residual Sums of Squares (RSS)- It used to measure the amount of variance in a data set that is not explained by a regression model. It measures the overall difference between the observed data, and the values predicted (or fitted) by the estimation model. The lower the value, the better the model $$\sum_{n=1}^ne_i^2$$

In r, we get the above information using the Analysis of Variance function *anova()* as follows;

```{r}

anova(mlr2)
```

The package *qpcR* in r also have very important functions that are useful in regression analysis.

## 3. Regression Model Diagonistics

After performing regression analysis, it is important to check whether the model works well for the data in hand. This chapter will explore different ways to check the accuracy of the model. It is important to evaluate how well the model fits the data because it helps you check whether the linear regression assumptions have been met or not. For instance, linear regression assumes that there is a linear relationship between the predictor variable and the response variable which might not be the case. The relationship might be polynomial or logarithmic. In addition, data might contain outliers or extreme values which may affect the regression. This is achieved by checking the distribution of the residual errors. Note that the predicted or the fitted values are the response variable values that you would expect for the given predictor variable values, according to the built regression model. From the scatter plot below, you can see that not all points fall exactly on the regression line. This means that for a given TV or Radio advertisement budget, the observed or the measured values can be different from the predicted or fitted values. The difference is known as **residual errors**, represented by the vertical red lines. The *augment* function from *broom* package gives several metrics useful in regression diagnostic. For easier explanation, I'll use the simple linear regression model.

```{r}
library(tidyverse)
library(broom)
library(ggplot2)
slrTVdiag<-augment(slrTV)
head(slrTVdiag)
```

The plot is as follows;

```{r}
PLOT2<-ggplot(slrTVdiag,aes(`TV Ad Budget ($)`,`Sales ($)`))+geom_point()+geom_segment(aes(xend=`TV Ad Budget ($)`,yend=.fitted),col="red",size=.3)+stat_smooth(method = lm,se=FALSE)
PLOT2
```

As mentioned earlier, the linear regression assumption are linearity, normality of residuals, Homogeneity of residual variance and the independence of the residual error terms.

### 3.1 Diagnostic Plot.

The base function *plot()* or the *autoplot()* function from *ggfortify* package can be used to plot regression diagnostic plots as follows;

```{r}
library(ggfortify)
autoplot(slrTV)

```

**1.The Residual vs Fitted plot**- It used to check linear relationship assumption. An approximate horizontal line without distinct pattern is a good indication of linear relationship.

**2. The Normal Q-Q Plot**- This plot is used to check whether the residuals are normally distributed. The residual terms should follow the straight dashed line to satisfy the assumption.

\*\**3. The Scale-Location Plot*- It used to check whether the residuals have a constant variance( homoscedasticity). A horizontal line with equally spread points is an indication of a constant variance which is not the case in our plot. The plot indicates that the variance of the residuals is heteroscedastic, which should be dealt with.

**4. Residuals vs Leverage Plot**- It is used to check extreme values that may affect the regression.Outliers may affect the interpretation of the model since they increase the RSE of the model.

For our case, the plot shows that there is linear relationship and that the residuals are normally distributed. Let us check the high leverage values and influential values.

```{r}
plot(slrTV,5)
```

A data point has a high leverage if it has an extreme predictor variable values. A data point above the statistic

$$\frac{2(p+1)}{n}$$

(where p is the number of predictors and n is the number of observations) indicates an observation with high leverage. In our case, the statistic is

$$\frac{2*2}{200}=0.02$$

The plot above indicates outliers on the 26, 36 and 179 which have a standardized error below -2, however none exceed a standard deviation of 3. All the observations are below 0.02, hence there are no observations with high leverage.

An influential value is a value which may alter the regression if it is included or excluded in the building of model. Note that not all ouliers are influential values. An observation has influence if its Cook's distance (P. Bruce and Bruce 2017) exceeds;

$$\frac{4}{n-p-1}$$.

```{r}
par(mfrow=c(1,2))
plot(slrTV,4)
plot(slrTV,5)
```

In our case, we do not have influential values, the cooks distance (represented by a red dotted line) is not drawn in the above plot because all the observations are well within the Cooks distance.

From the plot below, there is heteroscedasticity. This can be eliminated by transforming the data in various ways such as log transformation of the response variable.

```{r}
plot(slrTV,3)
```

```{r}
slrTVlog<-lm(log(`Sales ($)`)~`TV Ad Budget ($)`,data = AdvertisingBudgetandSales)
plot(slrTVlog,3)
```

From the plot above, the variance of the residual is homoscedastic.

## 4 Interaction Effect in Multiple Linear Regression

The multiple linear regression $sales=2.921+0.046TV+0.188Radio$ is also known as an **additive model**. This model only investigates the main effects of the model, where the assumption is that the relationship between one predictor variable and the response variable is independent of the other predictor variables. For example, in the above model, the effect on sales due to TV advertisement is independent of Radio advertisement which might not be true. It can be the case that spending money on TV advertisement may also increase the Radio advertisement effectiveness. In business, this is known as **synergy** while in statistics it is known as *interaction effect*. Generally, the model is written as (assuming we have two independent variables);

$$\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_1+\hat{\beta_2}x_2+\hat{\beta_3}x_1x_2$$

In r, we can build an interaction model as follows;

```{r}
mlrinteract<-lm(`Sales ($)`~`TV Ad Budget ($)`+`Radio Ad Budget ($)`+`TV Ad Budget ($)`:`Radio Ad Budget ($)`,data = AdvertisingBudgetandSales)


```

Or alternatively

```{r}

mlrinteract<-lm(`Sales ($)`~`TV Ad Budget ($)`*`Radio Ad Budget ($)`,data = AdvertisingBudgetandSales)
## Both ways will build the interaction model
##Let us have glimpse of the model metrics

summary(mlrinteract)
```

From the output above it can be seen that all the coefficients including the interaction coefficient are statistically significant (Note: If the interaction effect is statistically significant, do not try to interpret the predictor variables independently). The model is;

$$sales=6.750220 +0.019101TV+0.028860Radio+  0.001086TV*Radio$$

This interaction is known as **two way interaction** because it is interaction between two independent variables. High order interaction is possible also.

If the Radio advertisement budget is zero, then;

$$sales=6.750+0.019TV$$

The above implies that if Radio budget is zero, then TV advertisement causes an average of 0.019\*1000 dollars change in sales

However, if the Radio advertisement budget is one (or \$1000 for these case), then;

$$sales=6.750+0.019TV+0.029Tv$$

Therefore;

$$sales=6.750+0.048TV$$

The above implies that a budget of 1000 dollars in Radio advertisement, causes a 0.048\*1000 dollars change in sales.

A positive interaction in this case implies that the larger the Radio advertisement budget, the higher the effect of TV advertisement on the sales and similary, the larger the TV advertisement budget, the higher the effect of Radio advertisement on sales.

### 4.1 Additive and Interaction Models Comparison

The **Root Mean Square Error (RMSE)** of the additive model is;

```{r}
library(qpcR)
RMSE(mlr2)

```

While the RMSE of the interaction model is;

```{r}
RMSE(mlrinteract)
```

The lower the RMSE, the better the model. RMSE is the standard deviation of the residuals. It is a metric that tells us the average distance between the predicted or the fitted values and the observed or measured values. It is calculated as;

$$RMSE=\sqrt{\frac{\sum_{i=1}^n P_i-O_i}{n-1}} $$

Also the RMSE is the square root of the Mean Square Error(MSE) where MSE is the average of the squared differences between the observed values and the predicted values. For example;

```{r}
anova(mlrinteract)

##MSE = 0.9 implying that RMSE=sqrt(0.9)

sqrt(0.9)
RMSE(mlrinteract)
```

The Residual Standard Error or the model sigma is the variant of RMSE adjusted for the number of predictors. Therefore, since the interaction model has lower RMSE, it is the best model.

Also, the $adjusted\space R^2$ of the interaction model is 0.9673 which is equivalent to 96.73%, while that of the additive model is 0.8962 or 89.62%, implying that the interactive model is better than the additive model, since 96.73% of the total variation in sales is explained by the interactive model.

## 5 Regression Model Validation.

The commonly used metrics for validation of a regression model are;

1.  Root Mean Square Error (RMSE)

2.  Residual Standard Error (RSE)

3.  R-squared ($R^2$)

4.  Mean Absolute Error (MAE)

5.  Akaike Information Criterion (AIC) or AICc

6.  Bayesian Information Criterion (BIC)

The first three metrics have already been discussed above.

### 5.1 Akaike Information Criterion (AIC)

AIC was developed by a Japanese statistician Hirotugu Akaike, in 1970. AIC penalizes the inclusion of additional variables in the model. It is used to compare various models of the same data and determine which model is the best. The best model is the model with the lowest AIC. Notice that adding more parameters increases the AIC, hence the model with fewer parameters will have the lower AIC. [^2]

[^2]: Cavanaugh, J. E., & Neath, A. A. (2019). The Akaike information criterion: Background, derivation, properties, application, interpretation, and refinements. Wiley Interdisciplinary Reviews: Computational Statistics, 11(3), e1460.

According to AIC, the best model is the model which explains the greatest amount of variation in the dependent variable using the fewest possible independent variables. It is calculated as;

$$AIC=2k-2\ln{L}$$

where;

\-$k$ is the number of independent variables.

\-$L$ is the log-likelihood estimate (Likelihood that your model would have produced the observed model)

The default number of independent variables is 2, so if you have one independent variable, then, $k=3$ and so on.

To use the AIC, you need to build several models and then compare them. For example, we may build several models using the advertisement data set and see which best explains the variations in sale. In this case, I will build separate models for each independent variable, and then compare it with the model for the combined independent variables (TV, Radio and Newspaper). I had already done models for TV advert then TV, Radio and Newspaper advert, as well as TV, Radio advert, hence I'll just proceed to the remaining two separate models for Radio and Newspaper.

```{r}

slrRadio<-lm(`Sales ($)`~`Radio Ad Budget ($)`,data = AdvertisingBudgetandSales)
slrNewspaper<-lm(`Sales ($)` ~`Newspaper Ad Budget ($)`  ,data = AdvertisingBudgetandSales)

```

For clarification, I have named the models as follows;

-   slrTV - TV advertisement

-   slrRadio - Radio Advertisement

-   slrNewspaper - Newspaper advertisement

-   mlr - model for combined TV, Radio and Newspaper advert

-   mlr2 - combined model for TV and Radio adverts without newspaper

-   mlrinteract - interaction model

I'm going to use the function *aictab()* from the *AICcmodavg* package as follows;

```{r}
##listing the models in a list
Models<-list(slrTV,slrRadio,slrNewspaper,mlr,mlr2,mlrinteract)
##naming the models
Models.names=c("slrTV","slrRadio","slrNewspaper","mlr","mlr2","mlrinteract")
library(AICcmodavg)
##Then use the function
aictab(cand.set=Models, modnames =  Models.names)
```

The best fit model is always listed first. The best model for this study is the Interaction model (Recall that it is the interaction between TV and Radio Adverts). The AICc in the output above contains the model information -the lower the value the better the model. The lowercase "c" implies that it is the AIC of small samples. The Delta_AICc (or Delta_AIC) is the difference between the AICc (or AIC) of the best model and the model being compared. LL is the log likelihood used to calculate the AIC.

### 5.2 Bayesian Information Criterion

It is a method for scoring and selecting a model. It is almost similar to AIC,only that while AIC penalizes the additional parameters, BIC penalizes the complexity.More complex models have higher BICs. The lower the value, the better the model. It is widely used in *logistic Regression* It is calculated as;

$$BIC=-2L+\ln{N}*K$$

Where; -$k$ is the number of independent variables.

\-$L$ is the log-likelihood estimate (Likelihood that your model would have produced the observed model) - $N$ is the number of observations.

**Note that AIC and BIC are best used in models fit by Maximum Likelihood Estimation framework**

### 5.3 Mean Absolute Error (MAE)

MAE is a loss function used for regression. The loss is the mean over the absolute differences of the observed values and the predicted values. The lower the value the better the model. It is calculated as;

$$MAE=\frac{1}{N}\sum_{i=1}^N|y-\hat{y_i}|$$
